{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seinfield Analysis:   Data Acquisition\n",
    "\n",
    "## DATA 620 Web Analytics, CUNY Spring 2018\n",
    "\n",
    "__Team:__ Andy Carson, Nathan Cooper, Walt Wells\n",
    "\n",
    "### Data Source:  Seinology Scripts\n",
    "\n",
    "http://www.seinology.com/scripts.shtml\n",
    "\n",
    "### Scrape Inspirations\n",
    "\n",
    "* https://github.com/amanthedorkknight/the-seinfeld-chronicles\n",
    "* https://data.world/rickyhennessy/seinfeld-scripts/workspace/file?filename=seinfeld_scrape.py\n",
    "\n",
    "### What else did we do?\n",
    "\n",
    "In order to facilitate SNA, we extracted additional information regarding scenes and dialogue order in scenes.   The data is organized and saved as follows:  \n",
    "\n",
    "*  Seinfield_Metadata.csv:   Episode metadata - KEY: Ep/SeasonNum, AirDate, EpTitle, Director\n",
    "*  Seinfield_Cast.csv:   Cast - KEY Ep/SeasonNum,  Actor Name, Character\n",
    "*  Seinfield_Writers.csv:   Writers - KEY Ep/SeasonNum,  Writer Name\n",
    "*  Seinfield_Dialogue.csv:   Dialogue - KEY Ep/SeasonNum,  Character,  Text, SceneNum, DialogueIndex \n",
    "\n",
    "Of particular interest here is the organization of the Script data.   The data is organized by scene so that SNA could potentially be performed showing when characters are interacting in a scene.   The Index helps show the order of the dialogue in the scene.  It resets for each scene.  \n",
    "\n",
    "### Episode Details\n",
    "\n",
    "We extracted all episodes except 100-101 and 177-178 which were clip shows.   They followed a different format and other than some short perfunctory introductions by main characters, their data is all contained in the original episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import urllib \n",
    "import requests\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.seinology.com/scripts/script-' \n",
    "episode_numbers = (\n",
    "    list(map(lambda n: '%02d' % n, range(1, 82))) +\n",
    "    ['82and83'] +\n",
    "    list(map(lambda n: '%02d' % n, range(84, 100))) +\n",
    "    list(map(lambda n: '%02d' % n, range(102, 177))) +\n",
    "    ['179and180']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Metadata, Writers, Cast, and Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGetter(episode_numbers, baseurl):\n",
    "    '''collect and organize all information around seinfield from seinology.com scripts'''\n",
    "    \n",
    "    def get_episode_html(no, base_url):\n",
    "        ## Helper for getting raw html\n",
    "        url = base_url + str(no) + '.shtml'\n",
    "        source_code = requests.get(url)\n",
    "        html = source_code.text\n",
    "        return html\n",
    "    \n",
    "    # initialize empty objects \n",
    "    metadatadf = pd.DataFrame([])\n",
    "    castdf = pd.DataFrame([])\n",
    "    writerdf = pd.DataFrame([])\n",
    "    dialoguedf = pd.DataFrame([])\n",
    "    \n",
    "    for episode in episode_numbers:\n",
    "        \n",
    "        \n",
    "        html = get_episode_html(episode, base_url)\n",
    "        groups = re.search(r'pc: .*? season (\\d+), episode (\\d+)', html).groups()\n",
    "        season_num = int(groups[0])\n",
    "        episode_num = int(groups[1])\n",
    "        seid = 'S'+ str('%02d' % season_num) + 'E' + str('%02d' % episode_num)\n",
    "        if episode == '01':\n",
    "            seid = 'S01E00'\n",
    "                                                             \n",
    "        print(\"Scraping Episode: %s\" % seid)                                                    \n",
    "        \n",
    "        html_split = re.split(r'={30}.*', html)\n",
    "        top = html_split[0]\n",
    "        if episode == '179and180':\n",
    "            script = html_split[2]\n",
    "        else:\n",
    "            script = html_split[1]\n",
    "        html_split2 = re.split(r'-{30}.*', top)\n",
    "        cast = html_split2[1]\n",
    "        \n",
    "        ### Get Metadata DF\n",
    "        title = re.search(r'Episode \\d+(.*?) - (.*?)<', html).groups()[1]\n",
    "        title = re.sub(r'[^\\x00-\\x7f]',r'', title)\n",
    "        date = re.search(r'Broadcast date: (.*?)<', html).groups()[0]\n",
    "        director = re.search(r'Directed [bB]y (.*?)<', html).groups()[0]\n",
    "        \n",
    "        tempdf = pd.DataFrame({\"Season\": [season_num], \n",
    "                               \"Episode\": [episode_num],\n",
    "                               \"AirDate\": [date], \n",
    "                               \"Director\": [director],\n",
    "                               \"Title\": [title],\n",
    "                               \"SEID\": [seid]})\n",
    "        \n",
    "        metadatadf = metadatadf.append(tempdf, ignore_index=True)\n",
    "        \n",
    "        ### Get Writer DF\n",
    "        writers = re.search(r'Written [bB]y([:]|&nbsp;)? (.*?)<', html).groups()[1]\n",
    "        writers = ', '.join(tuple([w.strip() for w in re.split(r',|&amp;', writers) if w]))\n",
    "        writers = ', '.join(tuple([w.strip() for w in re.split(r'\\band\\b', writers) if w]))\n",
    "        writers = writers.split(',').strip()\n",
    "        writers = [''.join(x for x in par if x not in string.punctuation) for par in writers]\n",
    "        \n",
    "        tempdf = pd.DataFrame({\"Writers\": writers})\n",
    "        tempdf['SEID'] = seid\n",
    "        \n",
    "        writerdf = writerdf.append(tempdf, ignore_index=True)\n",
    "                                                             \n",
    "        ### Get Cast DF\n",
    "        castSoup = BeautifulSoup(cast)\n",
    "        castlist = list(filter(None, castSoup.find('body').text.replace('\\t', '').split('\\n')))\n",
    "        castlist = [j for i, j in enumerate(castlist) if j.find('...') > 0]\n",
    "        \n",
    "        Actor = []\n",
    "        Character = []\n",
    "        for c in castlist:\n",
    "            pair = c.split('..', 1)\n",
    "            Actor.append(pair[0].replace(u'\\xa0', u' ').replace(u'rc: ', u'').encode('utf-8').strip())\n",
    "            Character.append(pair[1].replace(u'\\xa0', u' ').encode('utf-8').replace('.', '').strip())\n",
    "\n",
    "        tempdf = pd.DataFrame({\"Actor\": Actor, \"Character\": Character})\n",
    "        tempdf['SEID'] = seid\n",
    "        \n",
    "        castdf = castdf.append(tempdf, ignore_index=True)\n",
    "        \n",
    "        ### Get Dialogue DF\n",
    "        soup = BeautifulSoup(script)\n",
    "        dialogues = list(filter(None, soup.find('body').text.replace('\\t', '').split('\\n')))\n",
    "        \n",
    "        script_df = pd.DataFrame([])\n",
    "        scene = 0\n",
    "        dialogueIndex = 1\n",
    "        sceneID = seid + '_' + str(1)\n",
    "        \n",
    "        for dialogue in dialogues:\n",
    "            if 'The End' in dialogue:\n",
    "                break\n",
    "            if episode in episode_numbers[0:11]:\n",
    "                if dialogue.isupper():\n",
    "                    scene += 1\n",
    "                    sceneID = seid + '_' + str(scene)\n",
    "            else: \n",
    "                if '[' in dialogue:\n",
    "                    scene += 1\n",
    "                    sceneID = seid + '_' + str(scene)\n",
    "            if (len(dialogue.split(':')) <= 1):\n",
    "                continue\n",
    "            if '[' in dialogue: \n",
    "                continue\n",
    "            if dialogue.isupper():\n",
    "                continue\n",
    "            dialogue_split = dialogue.split(':')\n",
    "            character = dialogue_split.pop(0).encode('utf-8').strip()\n",
    "            line = ''.join(dialogue_split).strip()\n",
    "            line = re.sub(r'[^\\x00-\\x7f]',r'', line).encode('utf-8').strip()\n",
    "            try: \n",
    "                if dialoguedf.SceneNum.iloc[-1] == sceneID:\n",
    "                    dialogueIndex += 1\n",
    "                else: \n",
    "                    dialogueIndex = 1\n",
    "            except:\n",
    "                pass\n",
    "            tempdf = pd.DataFrame({\"Character\": [character], \n",
    "                                   \"Dialogue\": [line],\n",
    "                                   \"SEID\": seid,\n",
    "                                   \"SceneNum\": sceneID,\n",
    "                                   \"DialogueIndex\": dialogueIndex})\n",
    "            dialoguedf = dialoguedf.append(tempdf, ignore_index = True)\n",
    "        \n",
    "    return metadatadf, writerdf, castdf, dialoguedf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Episode: S01E00\n",
      "Scraping Episode: S01E01\n",
      "Scraping Episode: S01E02\n",
      "Scraping Episode: S01E03\n",
      "Scraping Episode: S01E04\n",
      "Scraping Episode: S02E01\n",
      "Scraping Episode: S02E02\n",
      "Scraping Episode: S02E03\n",
      "Scraping Episode: S02E04\n",
      "Scraping Episode: S02E05\n",
      "Scraping Episode: S02E06\n",
      "Scraping Episode: S02E07\n",
      "Scraping Episode: S02E08\n",
      "Scraping Episode: S02E09\n",
      "Scraping Episode: S02E10\n",
      "Scraping Episode: S02E11\n",
      "Scraping Episode: S02E12\n",
      "Scraping Episode: S03E01\n",
      "Scraping Episode: S03E02\n",
      "Scraping Episode: S03E03\n",
      "Scraping Episode: S03E04\n",
      "Scraping Episode: S03E05\n",
      "Scraping Episode: S03E06\n",
      "Scraping Episode: S03E07\n",
      "Scraping Episode: S03E08\n",
      "Scraping Episode: S03E09\n",
      "Scraping Episode: S03E10\n",
      "Scraping Episode: S03E11\n",
      "Scraping Episode: S03E12\n",
      "Scraping Episode: S03E13\n",
      "Scraping Episode: S03E14\n",
      "Scraping Episode: S03E15\n",
      "Scraping Episode: S03E16\n",
      "Scraping Episode: S03E17\n",
      "Scraping Episode: S03E18\n",
      "Scraping Episode: S03E19\n",
      "Scraping Episode: S03E20\n",
      "Scraping Episode: S03E21\n",
      "Scraping Episode: S03E22\n",
      "Scraping Episode: S03E23\n",
      "Scraping Episode: S04E01\n",
      "Scraping Episode: S04E02\n",
      "Scraping Episode: S04E03\n",
      "Scraping Episode: S04E04\n",
      "Scraping Episode: S04E05\n",
      "Scraping Episode: S04E06\n",
      "Scraping Episode: S04E07\n",
      "Scraping Episode: S04E08\n",
      "Scraping Episode: S04E09\n",
      "Scraping Episode: S04E10\n",
      "Scraping Episode: S04E11\n",
      "Scraping Episode: S04E12\n",
      "Scraping Episode: S04E13\n",
      "Scraping Episode: S04E14\n",
      "Scraping Episode: S04E15\n",
      "Scraping Episode: S04E16\n",
      "Scraping Episode: S04E17\n",
      "Scraping Episode: S04E18\n",
      "Scraping Episode: S04E19\n",
      "Scraping Episode: S04E20\n",
      "Scraping Episode: S04E21\n",
      "Scraping Episode: S04E22\n",
      "Scraping Episode: S04E23\n",
      "Scraping Episode: S04E24\n",
      "Scraping Episode: S05E01\n",
      "Scraping Episode: S05E02\n",
      "Scraping Episode: S05E03\n",
      "Scraping Episode: S05E04\n",
      "Scraping Episode: S05E05\n",
      "Scraping Episode: S05E06\n",
      "Scraping Episode: S05E07\n",
      "Scraping Episode: S05E08\n",
      "Scraping Episode: S05E09\n",
      "Scraping Episode: S05E10\n",
      "Scraping Episode: S05E11\n",
      "Scraping Episode: S05E12\n",
      "Scraping Episode: S05E13\n",
      "Scraping Episode: S05E14\n",
      "Scraping Episode: S05E15\n",
      "Scraping Episode: S05E16\n",
      "Scraping Episode: S05E17\n",
      "Scraping Episode: S05E18\n",
      "Scraping Episode: S05E20\n",
      "Scraping Episode: S05E21\n",
      "Scraping Episode: S05E22\n",
      "Scraping Episode: S06E01\n",
      "Scraping Episode: S06E02\n",
      "Scraping Episode: S06E03\n",
      "Scraping Episode: S06E04\n",
      "Scraping Episode: S06E05\n",
      "Scraping Episode: S06E06\n",
      "Scraping Episode: S06E07\n",
      "Scraping Episode: S06E08\n",
      "Scraping Episode: S06E09\n",
      "Scraping Episode: S06E10\n",
      "Scraping Episode: S06E11\n",
      "Scraping Episode: S06E12\n",
      "Scraping Episode: S06E13\n",
      "Scraping Episode: S06E16\n",
      "Scraping Episode: S06E17\n",
      "Scraping Episode: S06E18\n",
      "Scraping Episode: S06E19\n",
      "Scraping Episode: S06E20\n",
      "Scraping Episode: S06E21\n",
      "Scraping Episode: S06E22\n",
      "Scraping Episode: S06E23\n",
      "Scraping Episode: S06E24\n",
      "Scraping Episode: S07E01\n",
      "Scraping Episode: S07E02\n",
      "Scraping Episode: S07E03\n",
      "Scraping Episode: S07E04\n",
      "Scraping Episode: S07E05\n",
      "Scraping Episode: S07E06\n",
      "Scraping Episode: S07E07\n",
      "Scraping Episode: S07E08\n",
      "Scraping Episode: S07E09\n",
      "Scraping Episode: S07E10\n",
      "Scraping Episode: S07E11\n",
      "Scraping Episode: S07E12\n",
      "Scraping Episode: S07E13\n",
      "Scraping Episode: S07E14\n",
      "Scraping Episode: S07E15\n",
      "Scraping Episode: S07E16\n",
      "Scraping Episode: S07E17\n",
      "Scraping Episode: S07E18\n",
      "Scraping Episode: S07E19\n",
      "Scraping Episode: S07E20\n",
      "Scraping Episode: S07E21\n",
      "Scraping Episode: S07E22\n",
      "Scraping Episode: S07E23\n",
      "Scraping Episode: S07E24\n",
      "Scraping Episode: S08E01\n",
      "Scraping Episode: S08E02\n",
      "Scraping Episode: S08E03\n",
      "Scraping Episode: S08E04\n",
      "Scraping Episode: S08E05\n",
      "Scraping Episode: S08E06\n",
      "Scraping Episode: S08E07\n",
      "Scraping Episode: S08E08\n",
      "Scraping Episode: S08E09\n",
      "Scraping Episode: S08E10\n",
      "Scraping Episode: S08E11\n",
      "Scraping Episode: S08E12\n",
      "Scraping Episode: S08E13\n",
      "Scraping Episode: S08E14\n",
      "Scraping Episode: S08E15\n",
      "Scraping Episode: S08E16\n",
      "Scraping Episode: S08E17\n",
      "Scraping Episode: S08E18\n",
      "Scraping Episode: S08E19\n",
      "Scraping Episode: S08E20\n",
      "Scraping Episode: S08E21\n",
      "Scraping Episode: S08E22\n",
      "Scraping Episode: S09E01\n",
      "Scraping Episode: S09E02\n",
      "Scraping Episode: S09E03\n",
      "Scraping Episode: S09E04\n",
      "Scraping Episode: S09E05\n",
      "Scraping Episode: S09E06\n",
      "Scraping Episode: S09E07\n",
      "Scraping Episode: S09E08\n",
      "Scraping Episode: S09E09\n",
      "Scraping Episode: S09E10\n",
      "Scraping Episode: S09E11\n",
      "Scraping Episode: S09E12\n",
      "Scraping Episode: S09E13\n",
      "Scraping Episode: S09E14\n",
      "Scraping Episode: S09E15\n",
      "Scraping Episode: S09E16\n",
      "Scraping Episode: S09E17\n",
      "Scraping Episode: S09E18\n",
      "Scraping Episode: S09E19\n",
      "Scraping Episode: S09E20\n",
      "Scraping Episode: S09E23\n"
     ]
    }
   ],
   "source": [
    "metadatadf, writerdf, castdf, dialoguedf = DataGetter(episode_numbers, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatadf.to_csv('Data/Seinfield_Metadata.csv', index=False)\n",
    "writerdf.to_csv('Data/Seinfield_Writers.csv', index=False)\n",
    "castdf.to_csv('Data/Seinfield_Cast.csv', index=False)\n",
    "dialoguedf.to_csv('Data/Seinfield_Dialogue.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Additional Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers = pd.read_csv('Data/Seinfield_Writers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers['Writers'] = writers['Writers'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers['Writers'][writers['Writers'] == 'Larry Charles  Story By Marc Jaffe'] = \"Larry Charles\"\n",
    "writers['Writers'][writers['Writers'] == 'Buck Dancer Larry David pseudonym'] = \"Larry David\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Larry David            58\n",
       "Peter Mehlman          19\n",
       "Larry Charles          19\n",
       "Jerry Seinfeld         18\n",
       "Alec Berg              14\n",
       "Jeff Schaffer          14\n",
       "Tom Gammill            13\n",
       "Max Pross              13\n",
       "Andy Robin             13\n",
       "Gregg Kavet            11\n",
       "Spike Feresten          9\n",
       "David Mandel            9\n",
       "Jennifer Crittenden     6\n",
       "Carol Leifer            6\n",
       "Dan OKeefe              5\n",
       "Steve Koren             5\n",
       "Marjorie Gross          4\n",
       "Bill Masters            3\n",
       "Bruce Kirschbaum        3\n",
       "Bruce Eric Kaplan       3\n",
       "Steve ODonnell          2\n",
       "Elaine Pope             2\n",
       "Matt Goldman            2\n",
       "Tom Leopold             2\n",
       "Larry Levin             2\n",
       "Bob Shaw                2\n",
       "Jon Hayman              1\n",
       "Greg Daniels            1\n",
       "Andy Cowan              1\n",
       "Ron Hague               1\n",
       "Steve Lookner           1\n",
       "Darin Henry             1\n",
       "Don McEnery             1\n",
       "Charlie Rubin           1\n",
       "Steve Skrovan           1\n",
       "Jill Franklyn           1\n",
       "Lawrence H Levy         1\n",
       "Fred Stoller            1\n",
       "Sam Kass                1\n",
       "Name: Writers, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writers['Writers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "writers.to_csv('Data/Seinfield_Writers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('Data/Seinfield_Metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['Director'][metadata['Director'] == 'David&nbsp; Steinberg'] = \"David Steinberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv('Data/Seinfield_Metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = pd.read_csv('Data/Seinfield_Dialogue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dialogue = dialogue.drop(dialogue[dialogue['Character']=='(from the movie we hear this dialogue'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue['Character']=dialogue['Character'].str.replace(r\"\\(.*\\)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue.to_csv('Data/Seinfield_Dialogue.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
