{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA 620 Assignment 5.2 - Document Classification\n",
    "Team: Andy, Walt, and Nathan\n",
    "    \n",
    "This is an attempt to classify the Reuters Corpus of 10,000 news articles in 90 categories, with potential multiple categories per article. The model evaluation was too much for a personal computer to handle. Given the time constraints in the class we opted to use a Corpus with fewer categories (the Brown Corpus), then try to implement the code using a Big Data Solution (such as Spark using Databricks, as example), even though that is the more interesting and practicle solution.\n",
    "\n",
    "We included this draft of the Jupyter notebook for the sake of completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/21565', 'test/21567', 'test/21568', 'test/21570', 'test/21571', 'test/21573', 'test/21574', 'test/21575', 'test/21576', 'training/1', 'training/10', 'training/100', 'training/1000', 'training/10000', 'training/10002', 'training/10005', 'training/10008', 'training/10011', 'training/10014', 'training/10015']\n",
      "<class 'list'>\n",
      "10788\n"
     ]
    }
   ],
   "source": [
    "#Exploration of the corpus\n",
    "print(reuters.fileids()[3010:3030])\n",
    "print(type(reuters.fileids()))\n",
    "print(len(reuters.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/21570', 'test/21571', 'test/21573', 'test/21574', 'test/21575', 'test/21576']\n",
      "['training/1', 'training/10', 'training/100', 'training/1000', 'training/10000']\n",
      "0.38859570086240186\n"
     ]
    }
   ],
   "source": [
    "#Find the training test split\n",
    "test_fileIds = reuters.fileids()[0:3019]\n",
    "training_fileIds = reuters.fileids()[3019:]\n",
    "print(test_fileIds[3013:3019])\n",
    "print(training_fileIds[0:5])\n",
    "print(len(test_fileIds)/len(training_fileIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test/15023': ['earn']}\n",
      "{'training/1016': ['earn']}\n"
     ]
    }
   ],
   "source": [
    "test_categories = [{fileId:reuters.categories(fileId)} for fileId in test_fileIds]\n",
    "print(test_categories[100])\n",
    "\n",
    "train_categories = [{fileId:reuters.categories(fileId)} for fileId in training_fileIds]\n",
    "print(train_categories[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test/15023': ['CITYTRUST', 'BANCORP', 'INC', '&', 'lt', ';', 'CITR', ...]}\n",
      "{'training/1016': ['AMERICAN', 'STORES', '&', 'lt', ';', 'ASC', '>', ...]}\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [{fileId:reuters.words(fileId)} for fileId in test_fileIds]\n",
    "print(test_corpus[100])\n",
    "\n",
    "train_corpus = [{fileId:reuters.words(fileId)} for fileId in training_fileIds]\n",
    "print(train_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training/1016': ['AMERICAN', 'STORES', '&', 'lt', ';', 'ASC', '>', ...]}\n",
      "{'training/1016': ['american', 'store', 'lt', 'asc', 'see', 'lower', 'year', 'net', 'american', 'store', 'co', 'said', 'expects', 'report', 'earnings', 'per', 'share', 'dlrs', 'per', 'share', 'sale', 'slightly', 'billion', 'dlrs', 'year', 'ended', 'january', 'supermarket', 'chain', 'earned', 'dlrs', 'per', 'share', 'sale', 'billion', 'dlrs', 'last', 'year', 'company', 'elaborate']}\n",
      "{'test/15023': ['CITYTRUST', 'BANCORP', 'INC', '&', 'lt', ';', 'CITR', ...]}\n",
      "{'test/15023': ['citytrust', 'bancorp', 'inc', 'lt', 'citr', 'qtr', 'net', 'shr', 'dlrs', 'v', 'dlrs', 'net', 'v', 'avg', 'shrs', 'v']}\n"
     ]
    }
   ],
   "source": [
    "#Adapted from https://campus.datacamp.com/courses/natural-language-processing-fundamentals-in-python/\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    cleaned_corpus = []\n",
    "    for article in corpus:\n",
    "        dct = {}\n",
    "        for k,v in article.items():\n",
    "        \n",
    "            # Convert the tokens into lowercase: lower_tokens\n",
    "            lower_tokens = [t.lower() for t in v]\n",
    "        \n",
    "            # Retain alphabetic words: alpha_only - not sure if I want to keep.\n",
    "            alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "            # Remove all stop words: no_stops\n",
    "            no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "            # Instantiate the WordNetLemmatizer\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "            # Lemmatize all tokens into a new list: lemmatized\n",
    "            lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "        \n",
    "            dct[k] = lemmatized\n",
    "        cleaned_corpus.append(dct)\n",
    "   \n",
    "    return cleaned_corpus\n",
    "\n",
    "clean_train = clean_corpus(train_corpus)\n",
    "print(train_corpus[100])\n",
    "print(clean_train[100])\n",
    "clean_test = clean_corpus(test_corpus)\n",
    "print(test_corpus[100])\n",
    "print(clean_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['earn'], ['citytrust', 'bancorp', 'inc', 'lt', 'citr', 'qtr', 'net', 'shr', 'dlrs', 'v', 'dlrs', 'net', 'v', 'avg', 'shrs', 'v'])\n",
      "(['earn'], ['american', 'store', 'lt', 'asc', 'see', 'lower', 'year', 'net', 'american', 'store', 'co', 'said', 'expects', 'report', 'earnings', 'per', 'share', 'dlrs', 'per', 'share', 'sale', 'slightly', 'billion', 'dlrs', 'year', 'ended', 'january', 'supermarket', 'chain', 'earned', 'dlrs', 'per', 'share', 'sale', 'billion', 'dlrs', 'last', 'year', 'company', 'elaborate'])\n"
     ]
    }
   ],
   "source": [
    "def catagory_corpus(catagories,corpus):\n",
    "    cat_corp_list = []\n",
    "    \n",
    "    for index,dct in enumerate(catagories):\n",
    "        for k,v in dct.items():\n",
    "            corp_dct = corpus[index]\n",
    "            cat_corp_list.append((v,corp_dct[k]))\n",
    "    return cat_corp_list\n",
    "            \n",
    "cat_corp_test = catagory_corpus(test_categories,clean_test)\n",
    "print(cat_corp_test[100])\n",
    "\n",
    "cat_corp_train = catagory_corpus(train_categories,clean_train)\n",
    "print(cat_corp_train[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['growmark', 'daiwa', 'advisable', 'immunity', 'comtech', 'australia', 'coordinating', 'impression', 'vdo', 'diaz']\n"
     ]
    }
   ],
   "source": [
    "all_words_lower = [w.lower() for w in reuters.words()]\n",
    "#print(len(all_words_lower))\n",
    "\n",
    "alpha_only_all_words = [t for t in all_words_lower if t.isalpha()]\n",
    "\n",
    "            # Remove all stop words: no_stops\n",
    "no_stops_all_words = [t for t in alpha_only_all_words if t not in english_stops]\n",
    "\n",
    "\n",
    "            # Lemmatize all tokens into a new list: lemmatized\n",
    "word_features = [wordnet_lemmatizer.lemmatize(t) for t in no_stops_all_words]\n",
    "\n",
    "word_features = set(word_features)\n",
    "#all_words_treated = nltk.FreqDist(lemmatized_all_words)\n",
    "print(list(word_features)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rubber', 'jobs', 'acq', 'potato', 'heat', 'propane', 'coconut-oil', 'lin-oil', 'wheat', 'veg-oil', 'soy-oil', 'interest', 'naphtha', 'cotton-oil', 'earn', 'coffee', 'meal-feed', 'soy-meal', 'sunseed', 'castor-oil', 'rand', 'hog', 'wpi', 'palm-oil', 'copper', 'ship', 'dfl', 'orange', 'lei', 'silver', 'strategic-metal', 'tin', 'coconut', 'bop', 'carcass', 'instal-debt', 'palmkernel', 'rice', 'lumber', 'cpu', 'copra-cake', 'crude', 'rapeseed', 'tea', 'sun-oil', 'money-supply', 'groundnut', 'corn', 'ipi', 'jet', 'cocoa', 'platinum', 'dlr', 'yen', 'lead', 'groundnut-oil', 'nat-gas', 'nickel', 'dmk', 'barley', 'sorghum', 'gold', 'cotton', 'pet-chem', 'zinc', 'housing', 'money-fx', 'oilseed', 'nkr', 'trade', 'livestock', 'reserves', 'alum', 'gnp', 'gas', 'income', 'palladium', 'oat', 'l-cattle', 'sugar', 'retail', 'soybean', 'cpi', 'rape-oil', 'rye', 'sun-meal', 'grain', 'iron-steel', 'nzdlr', 'fuel'}\n"
     ]
    }
   ],
   "source": [
    "category_set = set(reuters.categories())\n",
    "print(category_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains(elaborate) True\n",
      "contains(store) True\n",
      "contains(asc) True\n",
      "contains(lt) True\n",
      "contains(ended) True\n",
      "contains(earnings) True\n",
      "contains(company) True\n",
      "contains(last) True\n",
      "contains(year) True\n",
      "contains(lower) True\n",
      "contains(share) True\n",
      "contains(net) True\n",
      "contains(said) True\n",
      "contains(chain) True\n",
      "contains(january) True\n",
      "contains(per) True\n",
      "contains(dlrs) True\n",
      "contains(expects) True\n",
      "contains(american) True\n",
      "contains(report) True\n",
      "contains(slightly) True\n",
      "contains(billion) True\n",
      "contains(supermarket) True\n",
      "contains(sale) True\n",
      "contains(see) True\n",
      "contains(earned) True\n",
      "contains(co) True\n",
      "['earn']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['earn']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "7769\n",
      "3019\n"
     ]
    }
   ],
   "source": [
    "def document_features(document):\n",
    "    doc_set = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document)\n",
    "    return features\n",
    "\n",
    "def category_vector(cat_list):\n",
    "    cat_set = set(cat_list)\n",
    "    cat_vec = []\n",
    "    for cat in category_set:\n",
    "        if cat in cat_set:\n",
    "            cat_vec.append(1)\n",
    "        else:\n",
    "            cat_vec.append(0)\n",
    "    return str(cat_vec)\n",
    "\n",
    "#There are a lot of falses, so I am going to print trues only\n",
    "cat,doc = cat_corp_train[100]\n",
    "for k,v in document_features(doc).items():\n",
    "    if v == True:\n",
    "        print(k,v)\n",
    "    else:\n",
    "        continue\n",
    "print(cat)        \n",
    "print(category_vector(cat))\n",
    "\n",
    "cat,doc = cat_corp_test[100]\n",
    "print(cat) \n",
    "print(category_vector(cat))        \n",
    "        \n",
    "train_set = [(document_features(d),category_vector(c)) for (c,d) in cat_corp_train]\n",
    "print(len(train_set))\n",
    "test_set = [(document_features(d),category_vector(c)) for (c,d) in cat_corp_test]\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[100][1])\n",
    "print(test_set[100][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "#classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(test_set[100][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Classifier was able to classify test_set[100] correctly. The 1 is 15th in the vector, the same as the vector above. However, nltk.classify.accuracy() takes more than 8 hours to run. So we have some evidence that the code works, however evaluting the model seems out of practical reach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
